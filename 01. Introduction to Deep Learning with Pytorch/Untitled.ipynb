{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff3204d8-78e7-4f38-947a-92a501b06da4",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bfc4e7-a704-4bb3-a3c4-cd38b1459978",
   "metadata": {},
   "source": [
    "## Chapter 1 - Introduction to PyTorch, a Deep Learning Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8057243a-e4d1-44af-9a75-97a5dc963e1e",
   "metadata": {},
   "source": [
    "### Section 1.1 - Introduction to deep learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcd5d1c-1fd8-4952-acbf-e141f89bafdc",
   "metadata": {},
   "source": [
    "#### Getting started with PyTorch tensors\n",
    "Tensors are PyTorch's core data structure and the foundation of deep learning. They're similar to NumPy arrays but have unique features.\n",
    "\n",
    "Here you have a Python list named temperatures containing daily readings from two weather stations. Try converting this into a tensor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2427394c-f44b-47e7-8305-1ecb6af0c4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[72, 75, 78],\n",
      "        [70, 73, 76]])\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "\n",
    "temperatures = [[72, 75, 78], [70, 73, 76]]\n",
    "\n",
    "# Create a tensor from temperatures\n",
    "temp_tensor = torch.tensor(temperatures)\n",
    "\n",
    "print(temp_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364df9a2-0173-4110-9f17-a141b588e453",
   "metadata": {},
   "source": [
    "#### Checking and adding tensors\n",
    "\n",
    "While collecting temperature data, you notice the readings are off by two degrees. Add two degrees to the `temperatures` tensor after verifying its shape and data type with `torch` to ensure compatibility with the adjustment tensor.\n",
    "\n",
    "The torch library and the temperatures tensor are loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2efc88d1-c469-4aeb-b877-499b78cb9258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjustment shape: torch.Size([2, 3])\n",
      "Adjustment type: torch.int64\n",
      "Temperatures shape: torch.Size([2, 3])\n",
      "Temperatures type: torch.int64\n"
     ]
    }
   ],
   "source": [
    "adjustment = torch.tensor([[2, 2, 2], [2, 2, 2]])\n",
    "\n",
    "# Display the shape of the adjustment tensor\n",
    "print(\"Adjustment shape:\", adjustment.shape)\n",
    "\n",
    "# Display the type of the adjustment tensor\n",
    "print(\"Adjustment type:\", adjustment.dtype)\n",
    "\n",
    "print(\"Temperatures shape:\", temp_tensor.shape)\n",
    "print(\"Temperatures type:\", temp_tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf9e2fc5-277e-4473-8369-3510430ff657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected temperatures: tensor([[74, 77, 80],\n",
      "        [72, 75, 78]])\n"
     ]
    }
   ],
   "source": [
    "adjustment = torch.tensor([[2, 2, 2], [2, 2, 2]])\n",
    "\n",
    "# Add the temperatures and adjustment tensors\n",
    "corrected_temperatures = temp_tensor + adjustment\n",
    "print(\"Corrected temperatures:\", corrected_temperatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8eea95-59a5-4146-8b01-f6ee2d3f8910",
   "metadata": {},
   "source": [
    "### Section 1.2 - Neural networks and layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d7f75-2427-4c71-9726-ba941c2f0bd4",
   "metadata": {},
   "source": [
    "#### Linear layer network\n",
    "\n",
    "Neural networks often contain many layers, but most of them are linear layers. Understanding a single linear layer helps you grasp how they work before adding complexity.\n",
    "\n",
    "Apply a linear layer to an input tensor and observe the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f62233af-9273-4c39-abe2-05cac53f927d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5795,  0.3002]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.tensor([[0.3471, 0.4547, -0.2356]])\n",
    "\n",
    "# Create a Linear layer\n",
    "linear_layer = nn.Linear(\n",
    "                         in_features=3, \n",
    "                         out_features=2\n",
    "                         )\n",
    "\n",
    "# Pass input_tensor through the linear layer\n",
    "output = linear_layer(input_tensor)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28934d2-1a81-4251-a1b9-b5e807d1c189",
   "metadata": {},
   "source": [
    "### Section 1.3 - Hidden layers and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5de6ee-f760-464f-a0a1-eb9caad3acb6",
   "metadata": {},
   "source": [
    "#### Your first neural network\n",
    "\n",
    "It's time for you to implement a small neural network containing two linear layers in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "395dc920-58cf-476f-9a10-f588bd2ed74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.7470]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1]])\n",
    "\n",
    "# Create a container for stacking linear layers\n",
    "model = nn.Sequential(nn.Linear(8, 4),\n",
    "                nn.Linear(4, 1)\n",
    "                )\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb78f081-6e31-4cb7-982a-57ff94ae87be",
   "metadata": {},
   "source": [
    "#### Counting the number of parameters\n",
    "\n",
    "Deep learning models are famous for having a lot of parameters. With more parameters comes more computational complexity and longer training times, and a deep learning practitioner must know how many parameters their model has.\n",
    "\n",
    "In this exercise, you'll first calculate the number of parameters manually. Then, you'll verify your result using the .numel() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57f25c80-fc95-494d-adce-47a3e403c720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters in the model is 53\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(9, 4),\n",
    "                      nn.Linear(4, 2),\n",
    "                      nn.Linear(2, 1))\n",
    "\n",
    "total = 0\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "for params in model.parameters():\n",
    "    #print(params)\n",
    "    total += params.numel()\n",
    "  \n",
    "print(f\"The number of parameters in the model is {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd67e38-9748-4eb2-9152-f84b4e7cb865",
   "metadata": {},
   "source": [
    "## Chapter 2 - Neural Network Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843aad23-1e5f-45aa-a275-b6068c88dccd",
   "metadata": {},
   "source": [
    "### Section 2.1 - discovering activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b7bd21-58a5-46d8-85b8-c650dd578c0e",
   "metadata": {},
   "source": [
    "#### The sigmoid and softmax functions\n",
    "\n",
    "The sigmoid and softmax functions are key activation functions in deep learning, often used as the final step in a neural network.\n",
    "\n",
    "Sigmoid is for binary classification\n",
    "Softmax is for multi-class classification\n",
    "Given a pre-activation output tensor from a network, apply the appropriate activation function to obtain the final output.\n",
    "\n",
    "torch.nn has already been imported as nn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ee4c68d-6121-4690-96a4-e2f65bcc70c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9168]])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.tensor([[2.4]])\n",
    "\n",
    "# Create a sigmoid function and apply it on input_tensor\n",
    "sigmoid = nn.Sigmoid()\n",
    "probability = sigmoid(input_tensor)\n",
    "print(probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "749b700e-3ef8-410d-835b-4ba5542109eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2828e-01, 1.1698e-04, 5.7492e-01, 3.4961e-02, 1.5669e-01, 1.0503e-01]])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.tensor([[1.0, -6.0, 2.5, -0.3, 1.2, 0.8]])\n",
    "\n",
    "# Create a softmax function and apply it on input_tensor\n",
    "softmax = nn.Softmax(dim=1)\n",
    "probabilities = softmax(input_tensor)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a856084e-342c-4a4b-8fea-24890321047c",
   "metadata": {},
   "source": [
    "### Section 2.2 - running a forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce75928e-d115-4d03-82bd-4d9b668f214b",
   "metadata": {},
   "source": [
    "#### Building a binary classifier in PyTorch\n",
    "\n",
    "Recall that a small neural network with a single linear layer followed by a sigmoid function is a binary classifier. It acts just like a logistic regression.\n",
    "\n",
    "Practice building this small network and interpreting the output of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86e9542a-7d2f-4117-a938-86c3bde0bdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0035]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Implement a small neural network for binary classification\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(8,1),\n",
    "  nn.Sigmoid()\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3bbaa2-fafb-40c3-ae1d-1ef4e6ea7b74",
   "metadata": {},
   "source": [
    "#### From regression to multi-class classification\n",
    "\n",
    "The models you have seen for binary classification, multi-class classification and regression have all been similar, barring a few tweaks to the model.\n",
    "\n",
    "Start building a model for regression, and then tweak the model to perform a multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "796b89b7-9df8-47f2-82c4-82261b2d3e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3857, 0.2766, 0.2157, 0.1219]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Update network below to perform a multi-class classification with four labels\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 20),\n",
    "  nn.Linear(20, 12),\n",
    "  nn.Linear(12, 6),\n",
    "  nn.Linear(6, 4),\n",
    "  nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30877b72-e893-4508-9abb-3c7cbdb30423",
   "metadata": {},
   "source": [
    "### Section 2.3 - Using loss functions to assess model predictions\n",
    "\n",
    "#### Creating one-hot encoded labels\n",
    "One-hot encoding converts a single integer label into a vector with N elements, where N is the number of classes. This vector contains zeros and a one at the correct position.\n",
    "\n",
    "In this exercise, you'll manually create a one-hot encoded vector for y, and then use PyTorch to simplify the process. Your dataset has three classes (0, 1, 2).\n",
    "\n",
    "`numpy (np)`, `torch.nn.functional (F)`, and `torch` are already imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90c61980-8b6f-477b-8021-6d225bae8744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot vector using NumPy: [0 1 0]\n",
      "One-hot vector using PyTorch: tensor([0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "y = 1\n",
    "num_classes = 3\n",
    "\n",
    "# Create the one-hot encoded vector using NumPy\n",
    "one_hot_numpy = np.array([0, 1, 0])\n",
    "\n",
    "# Create the one-hot encoded vector using PyTorch\n",
    "one_hot_pytorch = F.one_hot(torch.tensor(y), num_classes=3)\n",
    "\n",
    "print(\"One-hot vector using NumPy:\", one_hot_numpy)\n",
    "print(\"One-hot vector using PyTorch:\", one_hot_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f308cd8-900c-4868-b7e4-cf389e835f8c",
   "metadata": {},
   "source": [
    "#### Calculating cross entropy loss\n",
    "\n",
    "Cross-entropy loss is a widely used method to measure classification loss. In this exercise, you’ll calculate cross-entropy loss in PyTorch using:\n",
    "\n",
    "- `y`: the ground truth label.\n",
    "- `scores`: a vector of predictions before softmax.\n",
    "\n",
    "Loss functions help neural networks learn by measuring prediction errors. Create a one-hot encoded vector for y, define the cross-entropy loss function, and compute the loss using scores and the encoded label. The result will be a single float representing the sample's loss.\n",
    "\n",
    "`torch`, `CrossEntropyLoss`, and `torch.nn.functional` as `F` have already been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5afff2c2-b37f-4a6a-9468-79f988066759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.0619, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "y = [2]\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), num_classes=4)\n",
    "\n",
    "# Create the cross entropy loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Calculate the cross entropy loss\n",
    "loss = criterion(scores.double(), one_hot_label.double())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7539d36d-e944-4457-afc2-315af368b00d",
   "metadata": {},
   "source": [
    "### Section 2.4 - Using derivatives to update model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16345c9b-e22b-4e9d-8047-d4d1128c02a5",
   "metadata": {},
   "source": [
    "#### Accessing the model parameters\n",
    "\n",
    "A PyTorch model created with the `nn.Sequential()` is a module that contains the different layers of your network. Recall that each layer parameter can be accessed by indexing the created model directly. In this exercise, you will practice accessing the parameters of different linear layers of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0936c8ff-e115-4dcf-9d01-6a3c91f22c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of the first layer: Parameter containing:\n",
      "tensor([[ 0.1617,  0.0397,  0.1497,  0.1700,  0.2119, -0.2094, -0.1081,  0.0210,\n",
      "         -0.1026, -0.0270, -0.1196,  0.2085,  0.1260,  0.1189,  0.2382, -0.1286],\n",
      "        [ 0.2177, -0.0268,  0.1033,  0.1875,  0.0241, -0.0741,  0.0349,  0.2313,\n",
      "          0.0304, -0.0983,  0.0252, -0.1887,  0.0382,  0.1333,  0.0295, -0.0638],\n",
      "        [-0.0875, -0.0446,  0.0326,  0.2142, -0.1323, -0.1868,  0.1845,  0.0855,\n",
      "          0.2117, -0.2131, -0.1945,  0.0839, -0.0864,  0.1829, -0.0882,  0.0728],\n",
      "        [-0.0076,  0.1848, -0.1924, -0.1329,  0.2388, -0.1941, -0.0439, -0.1929,\n",
      "          0.1133, -0.0860, -0.0498, -0.2002,  0.2145,  0.0910, -0.0765, -0.0572],\n",
      "        [-0.0004, -0.2412, -0.0775,  0.1716, -0.1473, -0.0396, -0.1608, -0.2016,\n",
      "         -0.1252,  0.0864, -0.0735,  0.0526, -0.2240,  0.0455, -0.0840,  0.2091],\n",
      "        [ 0.2143,  0.1801,  0.1207, -0.1909,  0.1965,  0.2219,  0.2411,  0.2268,\n",
      "         -0.0122, -0.0631, -0.1668, -0.1393, -0.0550,  0.2404, -0.1703,  0.0318],\n",
      "        [ 0.0495, -0.1235,  0.0796,  0.2117, -0.1326,  0.1995, -0.1062,  0.0966,\n",
      "          0.1268, -0.2336, -0.1320,  0.1413, -0.2253,  0.2138, -0.2191, -0.1050],\n",
      "        [ 0.1955, -0.1306,  0.0797,  0.0128,  0.1181,  0.1781, -0.0637,  0.1572,\n",
      "          0.1841, -0.0234,  0.1706,  0.0075, -0.0615,  0.1180,  0.1409, -0.0307]],\n",
      "       requires_grad=True)\n",
      "Bias of the second layer: Parameter containing:\n",
      "tensor([0.0786, 0.0470], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(16, 8),\n",
    "                      nn.Linear(8, 2)\n",
    "                     )\n",
    "\n",
    "# Access the weight of the first linear layer\n",
    "weight_0 = model[0].weight\n",
    "print(\"Weight of the first layer:\", weight_0)\n",
    "\n",
    "# Access the bias of the second linear layer\n",
    "bias_1 = model[1].bias\n",
    "print(\"Bias of the second layer:\", bias_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61879322-0487-4644-a204-bedfe9cf7124",
   "metadata": {},
   "source": [
    "#### Updating the weights manually\n",
    "\n",
    "Now that you know how to access weights and biases, you will manually perform the job of the PyTorch optimizer. While PyTorch automates this, practicing it manually helps you build intuition for how models learn and adjust. This understanding will be valuable when debugging or fine-tuning neural networks.\n",
    "\n",
    "A neural network of three layers has been created and stored as the model variable. This network has been used for a forward pass and the loss and its derivatives have been calculated. A default learning rate, lr, has been chosen to scale the gradients when performing the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "30828207-fa00-4125-afa0-dd573648587f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3071,  0.0870,  0.1546,  0.2295],\n",
      "        [-0.3298,  0.3811,  0.4934,  0.0941]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Input\n",
    "input_tensor = torch.randn((1, 16))\n",
    "y = torch.tensor([0])\n",
    "\n",
    "# Model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(16, 8),\n",
    "    nn.Linear(8, 4),\n",
    "    nn.Linear(4, 2)\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, y)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Manual weight update (no torch.no_grad, using .data)\n",
    "weight0 = model[0].weight\n",
    "weight1 = model[1].weight\n",
    "weight2 = model[2].weight\n",
    "\n",
    "weight0.data -= lr * weight0.grad\n",
    "weight1.data -= lr * weight1.grad\n",
    "weight2.data -= lr * weight2.grad\n",
    "print(weight2.data)\n",
    "\n",
    "# Don't forget to zero the gradients\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e428a0bd-9f02-480e-bc35-1e5a30980654",
   "metadata": {},
   "source": [
    "#### Using the PyTorch optimizer\n",
    "\n",
    "Earlier, you manually updated the weight of a network, gaining insight into how training works behind the scenes. However, this method isn’t scalable for deep networks with many layers.\n",
    "\n",
    "Thankfully, PyTorch provides the SGD optimizer, which automates this process efficiently in just a few lines of code. Now, you’ll complete the training loop by updating the weights using a PyTorch optimizer.\n",
    "\n",
    "A neural network has been created and provided as the model variable. This `model` was used to run a forward pass and create the tensor of predictions `pred`. The one-hot encoded tensor is named `target` and the cross entropy loss function is stored as `criterion`.\n",
    "\n",
    "`torch.optim` as `optim`, and `torch.nn` as `nn` have already been loaded for you."
   ]
  },
  {
   "cell_type": "raw",
   "id": "66dba24a-48e6-4c0e-bbbf-aa0e88eb6166",
   "metadata": {},
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "loss = criterion(output, y)\n",
    "loss.backward()\n",
    "\n",
    "# Update the model's parameters using the optimizer\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90a4fa0-686c-43f4-a0bd-373c754e32f9",
   "metadata": {},
   "source": [
    "## Chapter 3 - Training a Neural Network with PyTorch\n",
    "\n",
    "### Section 3.1 - A deeper dive into loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a53aa0b-148b-46e5-b6c7-8b9b807999a6",
   "metadata": {},
   "source": [
    "#### Using TensorDataset\n",
    "\n",
    "Structuring your data into a dataset is one of the first steps in training a PyTorch neural network. `TensorDataset` simplifies this by converting NumPy arrays into a format PyTorch can use.\n",
    "\n",
    "In this exercise, you'll create a `TensorDataset` using the preloaded `animals` dataset and inspect its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "19d6ab43-cfe1-48e1-91e8-72c25abb949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [['sparrow', 0, 1, 1, 0, 0, 2, 1, 0],\n",
    "       ['eagle', 0, 1, 1, 0, 1, 2, 1, 0],\n",
    "       ['cat', 1, 0, 0, 1, 1, 4, 1, 1],\n",
    "       ['dog', 1, 0, 0, 1, 0, 4, 1, 1],\n",
    "       ['lizard', 0, 0, 1, 0, 1, 4, 1, 2]]\n",
    "\n",
    "columns =['animal_name', 'hair', 'feathers', 'eggs', 'milk', 'predator', 'legs', 'tail', 'type']\n",
    "animals = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fb1cfd2f-6c83-453f-bd75-44e542a31712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sample: tensor([0, 1, 1, 0, 0, 2, 1])\n",
      "Label sample: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "X = animals.iloc[:, 1:-1].to_numpy()  \n",
    "y = animals.iloc[:, -1].to_numpy()\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(torch.tensor(X), torch.tensor(y))\n",
    "\n",
    "# Print the first sample\n",
    "input_sample, label_sample = dataset[0]\n",
    "print('Input sample:', input_sample)\n",
    "print('Label sample:', label_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff4741-2a38-4e5a-af96-77815cfe1865",
   "metadata": {},
   "source": [
    "#### Using DataLoader\n",
    "\n",
    "The `DataLoader` class is essential for efficiently handling large datasets. It speeds up training, optimizes memory usage, and stabilizes gradient updates, making deep learning models more effective.\n",
    "\n",
    "Now, you'll create a PyTorch `DataLoader` using the `dataset` from the previous exercise and see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "455c8b3c-4976-4682-ab3e-4373a0d83492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_inputs: tensor([[0, 1, 1, 0, 0, 2, 1],\n",
      "        [1, 0, 0, 1, 1, 4, 1]])\n",
      "batch_labels: tensor([0, 1])\n",
      "batch_inputs: tensor([[1, 0, 0, 1, 0, 4, 1],\n",
      "        [0, 0, 1, 0, 1, 4, 1]])\n",
      "batch_labels: tensor([1, 2])\n",
      "batch_inputs: tensor([[0, 1, 1, 0, 1, 2, 1]])\n",
      "batch_labels: tensor([0])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Iterate over the dataloader\n",
    "for batch_inputs, batch_labels in dataloader:\n",
    "    print('batch_inputs:', batch_inputs)\n",
    "    print('batch_labels:', batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9276356a-81d7-4453-bd8f-825d400c1f3b",
   "metadata": {},
   "source": [
    "### Section 3.2 - Writing our first training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b2356-511f-4d7c-94a5-e44dc1295128",
   "metadata": {},
   "source": [
    "#### Using the MSELoss\n",
    "\n",
    "For regression problems, you often use Mean Squared Error (MSE) as a loss function instead of cross-entropy. MSE calculates the squared difference between predicted values (`y_pred`) and actual values (`y`). Now, you'll compute MSE loss using both NumPy and PyTorch.\n",
    "\n",
    "`torch`, `numpy` (as `np`), and `torch.nn` (as `nn`) packages are already imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a510534b-1593-475f-ae00-236dc2a6f039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (NumPy): 0.375\n",
      "MSE (PyTorch): tensor(0.3750, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.array([3, 5.0, 2.5, 7.0])  \n",
    "y = np.array([3.0, 4.5, 2.0, 8.0])     \n",
    "\n",
    "# Calculate MSE using NumPy\n",
    "mse_numpy = np.mean((y-y_pred)**2)\n",
    "\n",
    "# Create the MSELoss function in PyTorch\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Calculate MSE using PyTorch\n",
    "mse_pytorch = criterion(torch.tensor(y_pred),torch.tensor(y))\n",
    "\n",
    "print(\"MSE (NumPy):\", mse_numpy)\n",
    "print(\"MSE (PyTorch):\", mse_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417a789d-d749-4871-bef6-6089fc331e3a",
   "metadata": {},
   "source": [
    "#### Writing a training loop\n",
    "\n",
    "In `scikit-learn`, the training loop is wrapped in the `.fit()` method, while in PyTorch, it's set up manually. While this adds flexibility, it requires a custom implementation.\n",
    "\n",
    "In this exercise, you'll create a loop to train a model for salary prediction.\n",
    "\n",
    "The `show_results()` function is provided to help you visualize some sample predictions.\n",
    "\n",
    "The package imports provided are: `pandas` as `pd`, `torch`, `torch.nn` as `nn`, `torch.optim` as `optim`, as well as `DataLoader` and `TensorDataset` from `torch.utils.data`.\n",
    "\n",
    "The following variables have been created: num_epochs, containing the number of epochs (set to 5); dataloader, containing the dataloader; model, containing the neural network; criterion, containing the loss function, nn.MSELoss(); optimizer, containing the SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ac452224-177a-4050-9c58-37a382fbf9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "75cc7063-f2cc-4017-ab44-0244f825e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a5d8d4ae-1de5-478d-91a7-d05a3164c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "X = animals.iloc[:, 1:-1].to_numpy()  \n",
    "y = animals.iloc[:, -1].to_numpy()\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float())\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimier = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ac9a90ad-2290-4ee8-a1b6-33cc78ecda3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(7, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7ab650a2-1357-4c11-8103-b7c88682feb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sacha/.cache/pypoetry/virtualenvs/datacamp-ml-M1zkPQRL-py3.10/lib/python3.10/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/sacha/.cache/pypoetry/virtualenvs/datacamp-ml-M1zkPQRL-py3.10/lib/python3.10/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# Loop over the number of epochs and the dataloader\n",
    "for i in range(num_epochs):\n",
    "  for data in dataloader:\n",
    "    # Set the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    # Run a forward pass\n",
    "    feature, target = data\n",
    "    prediction = model(feature)    \n",
    "    # Compute the loss\n",
    "    loss = criterion(prediction, target)    \n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "    # Update the model's parameters\n",
    "    optimizer.step()\n",
    "#show_results(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7502557f-92a3-4aa6-92c2-a106b6682563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
